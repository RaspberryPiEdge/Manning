{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert('RGB')\n",
        "    image = image.resize((224, 224), Image.BICUBIC)\n",
        "    image_array = np.array(image, dtype=np.float32) / 255.0\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "    return image_array\n",
        "\n",
        "def load_images_and_labels(data_directory):\n",
        "    images, labels = [], []\n",
        "    list_of_people = sorted([name for name in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, name)) and name != '.ipynb_checkpoints'])\n",
        "\n",
        "    for label, person in enumerate(list_of_people):\n",
        "        person_folder = os.path.join(data_directory, person)\n",
        "\n",
        "        for image_filename in os.listdir(person_folder):\n",
        "            if image_filename == '.ipynb_checkpoints' or not image_filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "            image_path = os.path.join(person_folder, image_filename)\n",
        "            image_array = preprocess_image(image_path)\n",
        "            images.append(image_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    images = np.concatenate(images, axis=0)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels, list_of_people\n",
        "\n",
        "data_directory = '/content/drive/MyDrive/faces-old'\n",
        "images, labels, list_of_people = load_images_and_labels(data_directory)\n",
        "\n",
        "num_classes = len(list_of_people)\n",
        "\n",
        "# Use MobileNetV2 model\n",
        "mobilenet_v2_model = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", output_shape=[1280], trainable=False)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "    mobilenet_v2_model,\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=20, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxpEEoJL5odf",
        "outputId": "eb3f726d-cf50-40b4-c404-51c830dd8349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 1s 610ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 1s 951ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 1s 587ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 1s 595ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 1s 580ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_person(image_path, model, list_of_people):\n",
        "    input_image = preprocess_image(image_path)\n",
        "    predictions = model.predict(input_image)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    return list_of_people[predicted_class_index]\n",
        "\n",
        "# Load a sample image for inference\n",
        "image_path = \"/content/drive/MyDrive/reader-test-person.png\"\n",
        "recognized_person = recognize_person(image_path, model, list_of_people)\n",
        "print(\"The person in the picture is:\", recognized_person)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CcA657uqfQQ",
        "outputId": "28d4b7a9-3938-4f9e-82b0-3e9b5810b29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "The person in the picture is: Logan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the representative dataset\n",
        "def representative_dataset():\n",
        "    for i in range(len(train_images)):\n",
        "        yield([train_images[i].reshape(1, 224, 224, 3)])\n",
        "\n",
        "# Convert the model to a quantized TensorFlow Lite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the quantized TensorFlow Lite model to a file\n",
        "with open('/content/drive/MyDrive/face_recognition_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "# Save the list_of_people to a CSV file\n",
        "with open('/content/drive/MyDrive/list_of_people.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for person in list_of_people:\n",
        "        writer.writerow([person])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Xwn73Id5oM",
        "outputId": "a6f99619-5285-4eaf-a336-83e17ff7a1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ZMtreEKE-i",
        "outputId": "3e85aa9e-3e8d-4006-caa8-70627d1b02b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2659  100  2659    0     0  98481      0 --:--:-- --:--:-- --:--:-- 98481\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Get:5 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6,332 B]\n",
            "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:9 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [2,317 B]\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,412 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,053 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,581 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,348 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,207 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,274 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,731 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,218 kB]\n",
            "Fetched 17.2 MB in 2s (6,908 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  edgetpu-compiler\n",
            "0 upgraded, 1 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 7,913 kB of archives.\n",
            "After this operation, 31.2 MB of additional disk space will be used.\n",
            "Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 16.0 [7,913 kB]\n",
            "Fetched 7,913 kB in 0s (50.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package edgetpu-compiler.\n",
            "(Reading database ... 122545 files and directories currently installed.)\n",
            "Preparing to unpack .../edgetpu-compiler_16.0_amd64.deb ...\n",
            "Unpacking edgetpu-compiler (16.0) ...\n",
            "Setting up edgetpu-compiler (16.0) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!edgetpu_compiler /content/drive/MyDrive/face_recognition_quant.tflite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BqXERwLKIC9",
        "outputId": "d252389c-9cbd-4f67-e7e1-849e996f6e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge TPU Compiler version 16.0.384591198\n",
            "Started a compilation timeout timer of 180 seconds.\n",
            "\n",
            "Model compiled successfully in 1367 ms.\n",
            "\n",
            "Input model: /content/drive/MyDrive/face_recognition_quant.tflite\n",
            "Input size: 2.74MiB\n",
            "Output model: face_recognition_quant_edgetpu.tflite\n",
            "Output size: 2.94MiB\n",
            "On-chip memory used for caching model parameters: 2.79MiB\n",
            "On-chip memory remaining for caching model parameters: 4.89MiB\n",
            "Off-chip memory used for streaming uncached model parameters: 64.00B\n",
            "Number of Edge TPU subgraphs: 1\n",
            "Total number of operations: 69\n",
            "Operation log: face_recognition_quant_edgetpu.log\n",
            "See the operation log file for individual operation details.\n",
            "Compilation child process completed within timeout period.\n",
            "Compilation succeeded! \n"
          ]
        }
      ]
    }
  ]
}
